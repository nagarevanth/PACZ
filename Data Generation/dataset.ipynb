{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n",
      "55\n",
      "torch.Size([55, 384])\n",
      "Top similar phrases:\n",
      "lot of charm: 0.3621\n",
      "confident speaker .: 0.3093\n",
      "Very confident speaker: 0.3040\n",
      "of charm: 0.3031\n",
      "confident speaker: 0.3022\n",
      "Very confident: 0.3010\n",
      "confident: 0.2991\n",
      "of charm .: 0.2770\n",
      "She: 0.2636\n",
      "charm: 0.2554\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Your inputs\n",
    "genz_meaning = \"charisma, ability to attract someone romantically\"\n",
    "corpus = [\"He has a lot of charm.\", \"She knows how to talk to people.\", \"He's got game.\", \"Very confident speaker.\"]\n",
    "\n",
    "# Get embedding of GenZ meaning\n",
    "genz_embedding = model.encode(genz_meaning, convert_to_tensor=True)\n",
    "print(genz_embedding.shape)\n",
    "\n",
    "# Create list of candidate phrases from corpus\n",
    "# You can improve this using n-grams, noun phrases, etc.\n",
    "phrases = []\n",
    "for sentence in corpus:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    phrases.extend(tokens)\n",
    "    phrases.extend([' '.join(gram) for n in range(2, 4) for gram in ngrams(tokens, n)])\n",
    "\n",
    "# Remove duplicates\n",
    "phrases = list(set(phrases))\n",
    "print(len(phrases))\n",
    "\n",
    "# Get embeddings\n",
    "phrase_embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "print(phrase_embeddings.shape)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_scores = util.pytorch_cos_sim(genz_embedding, phrase_embeddings)[0]\n",
    "\n",
    "# Sort\n",
    "top_results = np.argsort(-cos_scores)[:10]\n",
    "\n",
    "# Print top similar phrases\n",
    "print(\"Top similar phrases:\")\n",
    "for idx in top_results:\n",
    "    print(f\"{phrases[idx]}: {cos_scores[idx].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "Modified DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>content</th>\n",
       "      <th>Subject</th>\n",
       "      <th>modified_content</th>\n",
       "      <th>Modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phillip.allen@enron.co</td>\n",
       "      <td>tim.belden@enron.co</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phillip.allen@enron.co</td>\n",
       "      <td>john.lavorato@enron.co</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>Re</td>\n",
       "      <td>Traveling to have a business meeting takes Fac...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phillip.allen@enron.co</td>\n",
       "      <td>leah.arsdall@enron.co</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "      <td>Re: tes</td>\n",
       "      <td>test successful.  way to G2G!!!</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phillip.allen@enron.co</td>\n",
       "      <td>randall.gay@enron.co</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of Facts...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phillip.allen@enron.co</td>\n",
       "      <td>greg.piper@enron.co</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "      <td>Re: Hell</td>\n",
       "      <td>Let's shoot Ffs Tuesday at 11:45.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     From                      To   \n",
       "0  phillip.allen@enron.co     tim.belden@enron.co  \\\n",
       "1  phillip.allen@enron.co  john.lavorato@enron.co   \n",
       "2  phillip.allen@enron.co   leah.arsdall@enron.co   \n",
       "3  phillip.allen@enron.co    randall.gay@enron.co   \n",
       "4  phillip.allen@enron.co     greg.piper@enron.co   \n",
       "\n",
       "                                             content   Subject   \n",
       "0                          Here is our forecast\\n\\n        NaN  \\\n",
       "1  Traveling to have a business meeting takes the...        Re   \n",
       "2                     test successful.  way to go!!!   Re: tes   \n",
       "3  Randy,\\n\\n Can you send me a schedule of the s...       NaN   \n",
       "4                Let's shoot for Tuesday at 11:45.    Re: Hell   \n",
       "\n",
       "                                    modified_content  Modified  \n",
       "0                          Here is our forecast\\n\\n      False  \n",
       "1  Traveling to have a business meeting takes Fac...      True  \n",
       "2                    test successful.  way to G2G!!!      True  \n",
       "3  Randy,\\n\\n Can you send me a schedule of Facts...      True  \n",
       "4                Let's shoot Ffs Tuesday at 11:45.        True  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_phrases_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    phrases = set()\n",
    "\n",
    "    # Add noun chunks\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrases.add(chunk.text)\n",
    "\n",
    "    # Add named entities\n",
    "    for ent in doc.ents:\n",
    "        phrases.add(ent.text)\n",
    "\n",
    "    # Add unigrams and bigrams\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    for i in range(len(tokens)):\n",
    "        phrases.add(tokens[i])\n",
    "        if i < len(tokens) - 1:\n",
    "            phrases.add(f\"{tokens[i]} {tokens[i+1]}\")\n",
    "    return list(phrases)\n",
    "\n",
    "\n",
    "\n",
    "def similar_phrases_each_content(genz_embedding, genz_word, content):\n",
    "    # Return if content is not a string\n",
    "    if not isinstance(content, str):\n",
    "        return []\n",
    "\n",
    "    content = content.replace(\"\\n\", \"\")\n",
    "    content = content.replace(\"\\r\", \"\")\n",
    "\n",
    "    phrases = []\n",
    "    phrases = extract_phrases_spacy(content)\n",
    "    phrases = list(set(phrases))\n",
    "\n",
    "    if phrases == []:\n",
    "        return []\n",
    "    \n",
    "    phrase_embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "\n",
    "    cos_scores = util.pytorch_cos_sim(genz_embedding, phrase_embeddings)[0]\n",
    "\n",
    "    top_results = np.argsort(-cos_scores)[:10]\n",
    "\n",
    "    # Filter phrases with scores greater than 0.5\n",
    "    filtered_phrases = [(phrases[idx], cos_scores[idx].item()) for idx in top_results if cos_scores[idx].item() > 0.5]\n",
    "\n",
    "    return filtered_phrases\n",
    "\n",
    "\n",
    "def similar_phrase_content(genz_embedding, genz_word, df):\n",
    "    content = df['modified_content'].tolist()\n",
    "    modified_list = df['Modified'].tolist()\n",
    "    modified_contents = []\n",
    "    modified = []\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        phrases = similar_phrases_each_content(genz_embedding, genz_word, content[i])\n",
    "        modified_content = content[i]\n",
    "        modified_bool = modified_list[i]\n",
    "        for (phrase, score) in phrases:\n",
    "            modified_content = modified_content.replace(phrase, genz_word)\n",
    "            modified_bool = True\n",
    "\n",
    "        modified_contents.append(modified_content)\n",
    "        modified.append(modified_bool)\n",
    "\n",
    "    df['modified_content'] = modified_contents\n",
    "    df['Modified'] = modified\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/emails_cleaned.csv')\n",
    "\n",
    "df = df[['From', 'To', 'content', 'Subject']]\n",
    "\n",
    "genz = pd.read_csv('datasets/genz_slang.csv')\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# create a new column duplicate of content\n",
    "df['modified_content'] = df['content']\n",
    "df['Modified'] = False\n",
    "\n",
    "for i in range(len(genz)):\n",
    "    print(i)\n",
    "    genz_embedding = model.encode(genz.iloc[i, 1], convert_to_tensor=True)\n",
    "    similar_phrase_content(genz_embedding, genz.iloc[i, 0], df)\n",
    "\n",
    "# genz_embedding = model.encode(genz.iloc[4, 1], convert_to_tensor=True)\n",
    "# similar_phrase_content(genz_embedding, genz.iloc[4, 0], df)\n",
    "# df.iloc[1, 4]\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('datasets/emails_cleaned_modified.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame\n",
    "print(\"Modified DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\n",
      "\n",
      "As far as the business meetings, I think it would be more productive to try and stimulate discussions across the different Cheugy Cheugy working and Cheugy not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \n",
      "\n",
      "My suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.iloc[1, 4])\n",
    "df.iloc[1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       From                                               To   \n",
      "1    phillip.allen@enron.co                           john.lavorato@enron.co  \\\n",
      "2    phillip.allen@enron.co                            leah.arsdall@enron.co   \n",
      "3    phillip.allen@enron.co                             randall.gay@enron.co   \n",
      "4    phillip.allen@enron.co                              greg.piper@enron.co   \n",
      "6    phillip.allen@enron.co  david.l.johnson@enron.com, john.shafer@enron.co   \n",
      "..                      ...                                              ...   \n",
      "493  phillip.allen@enron.co                               mary.gray@enron.co   \n",
      "495  phillip.allen@enron.co                          patti.sullivan@enron.co   \n",
      "496  phillip.allen@enron.co                         andrea.richards@enron.co   \n",
      "497  phillip.allen@enron.co                            nick.politis@enron.co   \n",
      "499  phillip.allen@enron.co                        stagecoachmama@hotmail.co   \n",
      "\n",
      "                                               content   \n",
      "1    Traveling to have a business meeting takes the...  \\\n",
      "2                       test successful.  way to go!!!   \n",
      "3    Randy,\\n\\n Can you send me a schedule of the s...   \n",
      "4                  Let's shoot for Tuesday at 11:45.     \n",
      "6    Please cc the following distribution list with...   \n",
      "..                                                 ...   \n",
      "493  Griff,\\n  \\nCan you accomodate Dexter as we ha...   \n",
      "495  Patti,\\n\\nThis sounds like an opportunity to l...   \n",
      "496  I would support Matt Lenhart's promotion to th...   \n",
      "497  Nick,\\n\\nThere is a specific program that we a...   \n",
      "499  Lucy,\\n\\n#32 and #29 are fine.  \\n\\n#28 paid w...   \n",
      "\n",
      "                                             Subject   \n",
      "1                                                 Re  \\\n",
      "2                                            Re: tes   \n",
      "3                                                NaN   \n",
      "4                                           Re: Hell   \n",
      "6                                                NaN   \n",
      "..                                               ...   \n",
      "493                                 NGI access to eo   \n",
      "495               Analyst Interviews Needed - 2/15/0   \n",
      "496  Re: Associates & Analysts Eligible for Promotio   \n",
      "497                                        Re: Resum   \n",
      "499                                              NaN   \n",
      "\n",
      "                                      modified_content  Modified  \n",
      "1    Traveling to have a business meeting takes Fac...      True  \n",
      "2                      test successful.  way to G2G!!!      True  \n",
      "3    Randy,\\n\\n Can you send me a schedule of Facts...      True  \n",
      "4                  Let's shoot Ffs Tuesday at 11:45.        True  \n",
      "6    Please cc Facts following distribution list wi...      True  \n",
      "..                                                 ...       ...  \n",
      "493  Griff,\\n  \\nCan you accomodate Dexter Af we ha...      True  \n",
      "495  Patti,\\n\\nThis sounds like an opportunity to l...      True  \n",
      "496  I would support Matt Lenhart's promotion to Fa...      True  \n",
      "497  Nick,\\n\\nFactsre is a specific program that we...      True  \n",
      "499  Lucy,\\n\\n#32 and #29 are fine.  \\n\\n#28 paid w...      True  \n",
      "\n",
      "[287 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/emails_cleaned_modified.csv')\n",
    "\n",
    "modified_rows = df[df['Modified'] == True]\n",
    "# print(modified_rows['content'])\n",
    "# print(modified_rows['modified_content'])\n",
    "\n",
    "print(modified_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here is our forecast\\n\\n ', \"Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n\", 'test successful.  way to go!!!', 'Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip', \"Let's shoot for Tuesday at 11:45.  \"]\n",
      "Here is our forecast \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/emails_cleaned.csv')\n",
    "\n",
    "content_list = df['content'].to_list()\n",
    "\n",
    "print(content_list[:5])\n",
    "\n",
    "print(content_list[0].replace(\"\\n\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
